data:
  name: pl
  path: E:\pytorch-related\Pocket2Mol-main\Pocket2Mol_github_data\Pocket2Mol_github_data\crossdocked_pocket10
  split: E:\pytorch-related\Pocket2Mol-main\Pocket2Mol_github_data\Pocket2Mol_github_data\split_by_name.pt
  path_name2id: ./data/crossdocked_pocket10_name2id.pt
  path_sample: ./results
  pos_scale: 1.0    # multiply positions by this scalar
  feat_dim: 32         # the dimension of feature embeddings
  transform:
    ligand_atom_mode: add_aromatic_wo_h # basic
#    ligand_atom_mode: basic # basic
    random_rot: False

model:
  m_dim: 8  # old EGNN
  hidden_dim: 256
  use_mlp: True  # add one weight for the coordinate output
  num_timesteps: 1000  # number of time steps for the rectified flow model
  depth: 5   # number of EGNN layers
  gcl_layers: 1 # times of message passing
  agg_norm_factor: 5.0  # aggregation normalization factor
  feat_t_dim: 8
  egnn_attn: True #  early experiments tell us that attention does not help and it may degrade performance
  normalization: True   # We use layernorm instead of batch normalization.
  protein_ele_dim: 16
  protein_aa_dim: 16
  protein_is_backbone_dim: 8
train:
  n_steps_per_iter: 4
  loss_type: MAE # MAE or MSE
  time_sampling: 'uniform'  # [linear, importance, uniform] sample time_step with importance. To be specific, if a certain t gives a greater loss, then this t will be sampled more often
  len_history: 1000
  optimize_target_emb: False # optimize feature embeddings when it comes to target
  bond: True
  checkpoint: True
  ckp_path: configs/ckp_ERFM.pt
  epochs: 500
  batch_size: 1
  num_workers: 0
  max_grad_norm: 10.0
  ema_decay: 0.9999
  print_interval: 30
  distance_sin_emb: True
  shuffle: False
  optimize_embedding: False
  opt_time_emb: False
  n_knn: 48
  only_sample: False   # Note that before you set it as true, you have to set checkpoint as true.
  num_pockets: 5   # how many pockets we will use for sampling
  num_samples: 1
  optimizer:
    lr: 4.e-5
    weight_decay: 1.0e-3
  lr_scheduler:
    reset_lr: False
    stepsize: 1
    gamma: 0.8
    patience: 50
    lr_warmup_steps: 10000

sampling:
  num_spacing_steps: False  # False, 100, 50, 20, 10, 5
  pocket_id:
  bond_emb: True
  num_samples: 100
  cal_vina_score: True
  drop_unconnected_mol: True
  sampling_method: Euler
  batch_size: 100
  classifier_guidance: False
  cls_ckp_path: ''
  guidance_scale: 0.1
